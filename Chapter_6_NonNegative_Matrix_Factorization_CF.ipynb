{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M__y_Tpg4C6p"
      },
      "source": [
        "## Chapter 6 - Non-negative Matrix Factorization in Collaborative Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we start, we have to make sure that we already have our dependencies, that is 'recsys' folder"
      ],
      "metadata": {
        "id": "K-Ln-uc6FYYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#Check if we already have the 'recsys' folder\n",
        "if not (os.path.exists(\"recsys.zip\") or os.path.exists(\"recsys\")):\n",
        "    # If not then download directly from the source\n",
        "    !wget https://github.com/nzhinusoftcm/review-on-collaborative-filtering/raw/master/recsys.zip    \n",
        "    !unzip recsys.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SOicsSdFYmd",
        "outputId": "c01afc37-3465-4694-fc15-23bd6e5cbf7d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-03 21:08:21--  https://github.com/nzhinusoftcm/review-on-collaborative-filtering/raw/master/recsys.zip\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/nzhinusoftcm/review-on-collaborative-filtering/master/recsys.zip [following]\n",
            "--2023-01-03 21:08:21--  https://raw.githubusercontent.com/nzhinusoftcm/review-on-collaborative-filtering/master/recsys.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15312323 (15M) [application/zip]\n",
            "Saving to: ‘recsys.zip’\n",
            "\n",
            "recsys.zip          100%[===================>]  14.60M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-01-03 21:08:21 (252 MB/s) - ‘recsys.zip’ saved [15312323/15312323]\n",
            "\n",
            "Archive:  recsys.zip\n",
            "   creating: recsys/\n",
            "  inflating: recsys/datasets.py      \n",
            "  inflating: recsys/preprocessing.py  \n",
            "  inflating: recsys/utils.py         \n",
            "  inflating: recsys/requirements.txt  \n",
            "   creating: recsys/.vscode/\n",
            "  inflating: recsys/.vscode/settings.json  \n",
            "   creating: recsys/__pycache__/\n",
            "  inflating: recsys/__pycache__/datasets.cpython-36.pyc  \n",
            "  inflating: recsys/__pycache__/datasets.cpython-37.pyc  \n",
            "  inflating: recsys/__pycache__/utils.cpython-36.pyc  \n",
            "  inflating: recsys/__pycache__/preprocessing.cpython-37.pyc  \n",
            "  inflating: recsys/__pycache__/datasets.cpython-38.pyc  \n",
            "  inflating: recsys/__pycache__/preprocessing.cpython-36.pyc  \n",
            "  inflating: recsys/__pycache__/preprocessing.cpython-38.pyc  \n",
            "   creating: recsys/memories/\n",
            "  inflating: recsys/memories/ItemToItem.py  \n",
            "  inflating: recsys/memories/UserToUser.py  \n",
            "   creating: recsys/memories/__pycache__/\n",
            "  inflating: recsys/memories/__pycache__/UserToUser.cpython-36.pyc  \n",
            "  inflating: recsys/memories/__pycache__/UserToUser.cpython-37.pyc  \n",
            "  inflating: recsys/memories/__pycache__/ItemToItem.cpython-37.pyc  \n",
            "  inflating: recsys/memories/__pycache__/user2user.cpython-36.pyc  \n",
            "  inflating: recsys/memories/__pycache__/ItemToItem.cpython-36.pyc  \n",
            "   creating: recsys/models/\n",
            "  inflating: recsys/models/SVD.py    \n",
            "  inflating: recsys/models/MatrixFactorization.py  \n",
            "  inflating: recsys/models/ExplainableMF.py  \n",
            "  inflating: recsys/models/NonnegativeMF.py  \n",
            "   creating: recsys/models/__pycache__/\n",
            "  inflating: recsys/models/__pycache__/SVD.cpython-36.pyc  \n",
            "  inflating: recsys/models/__pycache__/MatrixFactorization.cpython-37.pyc  \n",
            "  inflating: recsys/models/__pycache__/ExplainableMF.cpython-36.pyc  \n",
            "  inflating: recsys/models/__pycache__/ExplainableMF.cpython-37.pyc  \n",
            "  inflating: recsys/models/__pycache__/MatrixFactorization.cpython-36.pyc  \n",
            "   creating: recsys/metrics/\n",
            "  inflating: recsys/metrics/EvaluationMetrics.py  \n",
            "   creating: recsys/img/\n",
            "  inflating: recsys/img/MF-and-NNMF.png  \n",
            "  inflating: recsys/img/svd.png      \n",
            "  inflating: recsys/img/MF.png       \n",
            "   creating: recsys/predictions/\n",
            "   creating: recsys/predictions/item2item/\n",
            "   creating: recsys/weights/\n",
            "   creating: recsys/weights/item2item/\n",
            "   creating: recsys/weights/item2item/ml1m/\n",
            "  inflating: recsys/weights/item2item/ml1m/similarities.npy  \n",
            "  inflating: recsys/weights/item2item/ml1m/neighbors.npy  \n",
            "   creating: recsys/weights/item2item/ml100k/\n",
            "  inflating: recsys/weights/item2item/ml100k/similarities.npy  \n",
            "  inflating: recsys/weights/item2item/ml100k/neighbors.npy  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Requirements\n",
        "Other than the 'recsys' folder, we also have to make sure that the other required libs have already been installed<br>\n",
        "```\n",
        "matplotlib==3.2.2\n",
        "numpy==1.19.2\n",
        "pandas==1.0.5\n",
        "python==3.7\n",
        "scikit-learn==0.24.1\n",
        "scikit-surprise==1.1.1\n",
        "scipy==1.6.2\n",
        "```\n",
        "(If we use Google Colab, most of these libs are already installed and up-to-date, except for *scikit-surprise* which is not pre-installed by Google Colab) "
      ],
      "metadata": {
        "id": "R5xNw81mFboA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Because *scikit-surprise* is required by current notebook, we're going to install *scikit-surprise* right away"
      ],
      "metadata": {
        "id": "udLPK4eGFi1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install surprise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kS4Iz2q3FlZT",
        "outputId": "74264b6c-3cfa-490b-e4f1-af316887b8cd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting surprise\n",
            "  Downloading surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
            "Collecting scikit-surprise\n",
            "  Downloading scikit-surprise-1.1.3.tar.gz (771 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.0/772.0 KB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-surprise->surprise) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-surprise->surprise) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-surprise->surprise) (1.7.3)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.3-cp38-cp38-linux_x86_64.whl size=2626456 sha256=0ce016051fb68798c19e8396c7c3bf405e080ba98c37f6e04646c0c17ae6315a\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/db/86/2c18183a80ba05da35bf0fb7417aac5cddbd93bcb1b92fd3ea\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise, surprise\n",
            "Successfully installed scikit-surprise-1.1.3 surprise-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all of the required libs"
      ],
      "metadata": {
        "id": "1wNc9dsnGBt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from recsys.preprocessing import mean_ratings\n",
        "from recsys.preprocessing import normalized_ratings\n",
        "from recsys.preprocessing import ids_encoder\n",
        "from recsys.preprocessing import train_test_split\n",
        "from recsys.preprocessing import rating_matrix\n",
        "from recsys.preprocessing import get_examples\n",
        "from recsys.preprocessing import scale_ratings\n",
        "\n",
        "from recsys.datasets import ml1m\n",
        "from recsys.datasets import ml100k\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import os"
      ],
      "metadata": {
        "id": "6Fy1KGU0F_Q9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and Preprocess the Movielens-100K Dataset"
      ],
      "metadata": {
        "id": "0x02RTm8HQkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load ml100k dataset\n",
        "ratings, movies = ml100k.load()\n",
        "\n",
        "# Encode userid and itemid in ratings\n",
        "ratings, uencoder, iencoder = ids_encoder(ratings)\n",
        "\n",
        "# Convert ratings from dataframe to numpy array\n",
        "np_ratings = ratings.to_numpy()\n",
        "\n",
        "# Get examples as tuples of userids and itemids and labels from raw ratings\n",
        "raw_examples, raw_labels = get_examples(ratings, labels_column=\"rating\")\n",
        "\n",
        "# Split the dataset to train set and test set\n",
        "(x_train, x_test), (y_train, y_test) = train_test_split(examples=raw_examples, labels=raw_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hL4a5keHP-i",
        "outputId": "3f10e8c9-914f-4cd7-f65f-085b0fa053ff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download data 100.2%\n",
            "Successfully downloaded ml-100k.zip 4924029 bytes.\n",
            "Unzipping the ml-100k.zip zip file ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that the data is prepared, let's move to the implementation of Non-negative Matrix Factorization"
      ],
      "metadata": {
        "id": "uAy9gxyNH2YM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NydAQqG4C6t"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nzhinusoftcm/review-on-collaborative-filtering/blob/master/6.NonNegativeMatrixFactorization.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZA1xuEx4C6u"
      },
      "source": [
        "# Non-negative Matrix Factorization for Recommendations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBDdEGx74C6u"
      },
      "source": [
        "Jusl like Matrix Factorization (MF) [(Yehuda Koren et al., 2009)](https://ieeexplore.ieee.org/document/5197422), Non-negative Matrix Factorization (NMF in short) factors the rating matrix $R$ in two matrices in such a way that $R = PQ^{\\top}$.\n",
        "\n",
        "### One limitation of Matrix Factorization\n",
        "\n",
        "$P$ and $Q$ values in MF are non interpretable since their components can take arbitrary (positive and negative) values.\n",
        "\n",
        "### Particulariy of Non-negative Matrix Factorization\n",
        "\n",
        "NMF [(Lee and Seung, 1999)](http://www.dm.unibo.it/~simoncin/nmfconverge.pdf) allows the reconstruction of $P$ and $Q$ in such a way that $P,Q \\ge 0$. Constraining $P$ and $Q$ values to be taken from $[0,1]$ allows  a probabilistic interpretation\n",
        "\n",
        "- Latent factors represent groups of users who share the same tastes,\n",
        "- The value  $P_{u,l}$  represents the probability that user $u$ belongs to the group $l$ of users and \n",
        "- The value $Q_{l,i}$ represents the probability that users in the group $l$  likes item $i$.\n",
        "\n",
        "### Objective function\n",
        "\n",
        "With the Euclidian distance, the NMF objective function is defined by\n",
        "\n",
        "\\begin{equation}\n",
        "J = \\frac{1}{2}\\sum_{(u,i) \\in \\kappa}||R_{u,i} - P_uQ_i^{\\top}||^2 + \\lambda_P||P_u||^2 + \\lambda_Q||Q_i||^2\n",
        "\\end{equation}\n",
        "\n",
        "The goal is to minimize the cost function $J$ by optimizing parameters $P$ and $Q$, with $\\lambda_P$ and $\\lambda_Q$ the regularizer parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwCUYusI4C6v"
      },
      "source": [
        "### Multiplicative update rule\n",
        "\n",
        "According [(Lee and Seung, 1999)](https://www.nature.com/articles/44565.pdf), to the multiplicative update rule for $P$ and $Q$ are as follows :\n",
        "\n",
        "\\begin{equation}\n",
        "P \\leftarrow P \\cdot \\frac{RQ}{PQ^{\\top}Q}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "Q \\leftarrow Q \\cdot \\frac{R^{\\top}P}{QP^{\\top}P}\n",
        "\\end{equation}\n",
        "\n",
        "However, since $R$ is a sparse matrix, we need to update each $P_u$ according to existing ratings of user $u$. Similarily, we need to update $Q_i$ according to existing ratings on item $i$. Hence :\n",
        "\n",
        "\\begin{equation}\n",
        "P_{u,k} \\leftarrow P_{u,k} \\cdot \\frac{\\sum_{i \\in I_u}Q_{i,k}\\cdot r_{u,i}}{\\sum_{i \\in I_u}Q_{i,k}\\cdot \\hat{r}_{u,i} + \\lambda_P|I_u|P_{u,k}}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "Q_{i,k} \\leftarrow Q_{i,k} \\cdot \\frac{\\sum_{u \\in U_i}P_{u,k}\\cdot r_{u,i}}{\\sum_{u \\in U_i}P_{u,k}\\cdot \\hat{r}_{u,i} + \\lambda_Q|U_i|Q_{i,k}}\n",
        "\\end{equation}\n",
        "\n",
        "Where\n",
        "- $P_{u,k}$ is the $k^{th}$ latent factor of $P_u$\n",
        "- $Q_{i,k}$ is the $k^{th}$ latent factor of $Q_i$\n",
        "- $I_u$ the of items rated by user $u$\n",
        "- $U_i$ the set of users who rated item $i$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKjl_eXQ4C6z"
      },
      "source": [
        "### Non-negative Matrix Factorization Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "u_4FkBs14C6z"
      },
      "outputs": [],
      "source": [
        "class NMF:\n",
        "    \n",
        "    def __init__(self, ratings, m, n, uencoder, iencoder, K=10, lambda_P=0.01, lambda_Q=0.01):\n",
        "        \n",
        "        np.random.seed(32)\n",
        "        \n",
        "        # initialize the latent factor matrices P and Q (of shapes (m,k) and (n,k) respectively) that will be learnt\n",
        "        self.ratings = ratings\n",
        "        self.np_ratings = ratings.to_numpy()\n",
        "        self.K = K\n",
        "        self.P = np.random.rand(m, K)\n",
        "        self.Q = np.random.rand(n, K)\n",
        "        \n",
        "        # hyper parameter initialization\n",
        "        self.lambda_P = lambda_P\n",
        "        self.lambda_Q = lambda_Q\n",
        "\n",
        "        # initialize encoders\n",
        "        self.uencoder = uencoder\n",
        "        self.iencoder = iencoder\n",
        "        \n",
        "        # training history\n",
        "        self.history = {\n",
        "            \"epochs\": [],\n",
        "            \"loss\": [],\n",
        "            \"val_loss\": [],\n",
        "        }\n",
        "    \n",
        "    def print_training_parameters(self):\n",
        "        print('Training NMF ...')\n",
        "        print(f'k={self.K}')\n",
        "        \n",
        "    def mae(self, x_train, y_train):\n",
        "        \"\"\"\n",
        "        returns the Mean Absolute Error\n",
        "        \"\"\"\n",
        "        # number of training examples\n",
        "        m = x_train.shape[0]\n",
        "        error = 0\n",
        "        for pair, r in zip(x_train, y_train):\n",
        "            u, i = pair\n",
        "            error += abs(r - np.dot(self.P[u], self.Q[i]))\n",
        "        return error / m\n",
        "    \n",
        "    def update_rule(self, u, i, error):\n",
        "        I = self.np_ratings[self.np_ratings[:, 0] == u][:, [1, 2]]\n",
        "        U = self.np_ratings[self.np_ratings[:, 1] == i][:, [0, 2]]    \n",
        "                    \n",
        "        num = self.P[u] * np.dot(self.Q[I[:, 0]].T, I[:, 1])\n",
        "        dem = np.dot(self.Q[I[:, 0]].T, np.dot(self.P[u], self.Q[I[:, 0]].T)) + self.lambda_P * len(I) * self.P[u]\n",
        "        self.P[u] = num / dem\n",
        "\n",
        "        num = self.Q[i] * np.dot(self.P[U[:, 0]].T, U[:, 1])\n",
        "        dem = np.dot(self.P[U[:, 0]].T, np.dot(self.P[U[:, 0]], self.Q[i].T)) + self.lambda_Q * len(U) * self.Q[i]\n",
        "        self.Q[i] = num / dem\n",
        "    \n",
        "    @staticmethod\n",
        "    def print_training_progress(epoch, epochs, error, val_error, steps=5):\n",
        "        if epoch == 1 or epoch % steps == 0:\n",
        "            print(f\"epoch {epoch}/{epochs} - loss : {round(error, 3)} - val_loss : {round(val_error, 3)}\")\n",
        "                \n",
        "    def fit(self, x_train, y_train, validation_data, epochs=10):\n",
        "\n",
        "        self.print_training_parameters()\n",
        "        x_test, y_test = validation_data\n",
        "        for epoch in range(1, epochs+1):\n",
        "            for pair, r in zip(x_train, y_train):\n",
        "                u, i = pair\n",
        "                r_hat = np.dot(self.P[u], self.Q[i])\n",
        "                e = abs(r - r_hat)\n",
        "                self.update_rule(u, i, e)                \n",
        "            # training and validation error  after this epochs\n",
        "            error = self.mae(x_train, y_train)\n",
        "            val_error = self.mae(x_test, y_test)\n",
        "            self.update_history(epoch, error, val_error)\n",
        "            self.print_training_progress(epoch, epochs, error, val_error, steps=1)\n",
        "        \n",
        "        return self.history\n",
        "    \n",
        "    def update_history(self, epoch, error, val_error):\n",
        "        self.history['epochs'].append(epoch)\n",
        "        self.history['loss'].append(error)\n",
        "        self.history['val_loss'].append(val_error)\n",
        "    \n",
        "    def evaluate(self, x_test, y_test):        \n",
        "        error = self.mae(x_test, y_test)\n",
        "        print(f\"validation error : {round(error,3)}\")\n",
        "        print('MAE : ', error)        \n",
        "        return error\n",
        "      \n",
        "    def predict(self, userid, itemid):\n",
        "        u = self.uencoder.transform([userid])[0]\n",
        "        i = self.iencoder.transform([itemid])[0]\n",
        "        r = np.dot(self.P[u], self.Q[i])\n",
        "        return r\n",
        "\n",
        "    def recommend(self, userid, N=30):\n",
        "        # encode the userid\n",
        "        u = self.uencoder.transform([userid])[0]\n",
        "\n",
        "        # predictions for users userid on all product\n",
        "        predictions = np.dot(self.P[u], self.Q.T)\n",
        "\n",
        "        # get the indices of the top N predictions\n",
        "        top_idx = np.flip(np.argsort(predictions))[:N]\n",
        "\n",
        "        # decode indices to get their corresponding itemids\n",
        "        top_items = self.iencoder.inverse_transform(top_idx)\n",
        "\n",
        "        # take corresponding predictions for top N indices\n",
        "        preds = predictions[top_idx]\n",
        "\n",
        "        return top_items, preds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ealfugdJ4C60"
      },
      "source": [
        "### Train the NMF model\n",
        "\n",
        "Selected model parameters :\n",
        "\n",
        "- $k = 10$ : (number of factors)\n",
        "- $\\lambda_P = 0.6$\n",
        "- $\\lambda_Q = 0.6$\n",
        "- epochs = 10\n",
        "\n",
        "*Note that it may take some time to complete the training on 10 epochs (around 7 minutes).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9bRFAYy4C60",
        "outputId": "fffe25ad-398a-48f9-864f-fe886bd4f44d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training NMF ...\n",
            "k=10\n",
            "epoch 1/10 - loss : 0.916 - val_loss : 0.917\n",
            "epoch 2/10 - loss : 0.915 - val_loss : 0.917\n",
            "epoch 3/10 - loss : 0.915 - val_loss : 0.917\n",
            "epoch 4/10 - loss : 0.915 - val_loss : 0.917\n",
            "epoch 5/10 - loss : 0.915 - val_loss : 0.917\n",
            "epoch 6/10 - loss : 0.915 - val_loss : 0.917\n",
            "epoch 7/10 - loss : 0.915 - val_loss : 0.917\n",
            "epoch 8/10 - loss : 0.915 - val_loss : 0.917\n",
            "epoch 9/10 - loss : 0.915 - val_loss : 0.917\n",
            "epoch 10/10 - loss : 0.915 - val_loss : 0.917\n"
          ]
        }
      ],
      "source": [
        "m = ratings['userid'].nunique()   # total number of users\n",
        "n = ratings['itemid'].nunique()   # total number of items\n",
        "\n",
        "# Create and train the model using train set\n",
        "nmf = NMF(ratings, m, n, uencoder, iencoder, K=10, lambda_P=0.6, lambda_Q=0.6)\n",
        "history = nmf.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the model using test set"
      ],
      "metadata": {
        "id": "5AA8wycUInbv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maG7Uwzo4C61",
        "outputId": "358130ec-f823-4822-83eb-9bc19dc7e80c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation error : 0.917\n",
            "MAE :  0.9165041343019539\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9165041343019539"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "nmf.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result seems okay because the MAE score is less than 1.0, but it's pretty near to the 1.0 so it can't be called a good result "
      ],
      "metadata": {
        "id": "lsxbzzcsI0Pn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaxchCim4C62"
      },
      "source": [
        "### Evaluation of NMF with Scikit-suprise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndo6zeGM4C62"
      },
      "source": [
        "We can use the [scikit-suprise](https://surprise.readthedocs.io/en/stable/) package to train the NMF model. It  is an easy-to-use Python scikit for recommender systems.\n",
        "\n",
        "1. Import the NMF class from the suprise scikit.\n",
        "2. Load the data with the built-in function\n",
        "3. Instanciate NMF with ```k=10``` (```n_factors```) and we use 10 epochs (```n_epochs```)\n",
        "4. Evaluate the model using cross-validation with 5 folds."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Dataset 1: ML-100K"
      ],
      "metadata": {
        "id": "7H0xk-F1Lhu0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Unfsgb14C62",
        "outputId": "be22dcad-f3aa-43d6-8a1c-cb64848345b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset ml-100k could not be found. Do you want to download it? [Y/n] Y\n",
            "Trying to download dataset from https://files.grouplens.org/datasets/movielens/ml-100k.zip...\n",
            "Done! Dataset ml-100k has been saved to /root/.surprise_data/ml-100k\n",
            "Evaluating MAE of algorithm NMF on 5 split(s).\n",
            "\n",
            "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
            "MAE (testset)     0.9626  0.9435  0.9777  0.9607  0.9510  0.9591  0.0116  \n",
            "Fit time          0.38    0.43    0.37    0.42    0.43    0.40    0.03    \n",
            "Test time         0.12    0.21    0.14    0.21    0.12    0.16    0.04    \n"
          ]
        }
      ],
      "source": [
        "from surprise import NMF\n",
        "from surprise import Dataset\n",
        "from surprise.model_selection import cross_validate\n",
        "\n",
        "# Load the movielens-100k dataset (download it if needed).\n",
        "data = Dataset.load_builtin('ml-100k')\n",
        "\n",
        "# Use the NMF algorithm.\n",
        "nmf = NMF(n_factors=10, n_epochs=10)\n",
        "\n",
        "# Run 5-fold cross-validation and print results.\n",
        "history = cross_validate(nmf, data, measures=['MAE'], cv=5, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RxCtkIK4C63"
      },
      "source": [
        "As result, the mean MAE on the test set is **mae = 0.9591** which is pretty close to the result we have obtained on *ml-100k* with our own implementation **mae = 0.9165**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It has a different MAE score due to the use of cross validation.<br>\n",
        "The use of cross validation is to ensure that the model is properly validated, so that the model doesn't easily show overfitting result"
      ],
      "metadata": {
        "id": "O4qLQXn7KSI5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EATVSzSt4C63"
      },
      "source": [
        "##### Dataset 2: ML-1M\n",
        "\n",
        "*This may take arount 2 minutes*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YPPO5Vh4C63",
        "outputId": "2ebdc8c0-90a9-4518-b58a-769b6806254a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset ml-1m could not be found. Do you want to download it? [Y/n] Y\n",
            "Trying to download dataset from https://files.grouplens.org/datasets/movielens/ml-1m.zip...\n",
            "Done! Dataset ml-1m has been saved to /root/.surprise_data/ml-1m\n",
            "Evaluating MAE of algorithm NMF on 5 split(s).\n",
            "\n",
            "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
            "MAE (testset)     0.9559  0.9580  0.9566  0.9639  0.9490  0.9567  0.0047  \n",
            "Fit time          3.53    3.84    3.86    3.83    3.79    3.77    0.12    \n",
            "Test time         2.60    2.37    2.33    2.34    2.03    2.33    0.18    \n"
          ]
        }
      ],
      "source": [
        "data = Dataset.load_builtin('ml-1m')\n",
        "nmf = NMF(n_factors=10, n_epochs=10)\n",
        "history = cross_validate(nmf, data, measures=['MAE'], cv=5, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The mean MAE on a 5-fold cross-validation is **mae = 0.9567**"
      ],
      "metadata": {
        "id": "S1LwwIb7Ly4o"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfLVWOwT4C63"
      },
      "source": [
        "## Reference\n",
        "\n",
        "1. Daniel D. Lee & H. Sebastian Seung (1999). [Learning the parts of objects by non-negative matrix factorization](https://www.nature.com/articles/44565)\n",
        "2. Deng Cai et al. (2008). [Non-negative Matrix Factorization on Manifold](https://ieeexplore.ieee.org/document/4781101)\n",
        "3. Yu-Xiong Wang and Yu-Jin Zhang (2011). [Non-negative Matrix Factorization: a Comprehensive Review](https://ieeexplore.ieee.org/document/6165290)\n",
        "4. Nicolas Gillis (2014). [The Why and How of Nonnegative Matrix Factorization](https://arxiv.org/pdf/1401.5226.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQcB-l6v4C64"
      },
      "source": [
        "## Author\n",
        "\n",
        "[Carmel WENGA](https://www.linkedin.com/in/carmel-wenga-871876178/), <br>\n",
        "PhD student at Université de la Polynésie Française, <br> \n",
        "Applied Machine Learning Research Engineer, <br>\n",
        "[ShoppingList](https://shoppinglist.cm), NzhinuSoft."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}